# meta developer: @mlwdev
# scope: hikka_only

import logging
import g4f
import asyncio

from .. import loader, utils
from telethon.tl.types import Message

logger = logging.getLogger(__name__)


@loader.tds
class G4fAskMod(loader.Module):
    """–ú–æ–¥—É–ª—å –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –ò–ò –º–æ–¥–µ–ª—è–º–∏ —á–µ—Ä–µ–∑ g4f"""
    strings = {
        "name": "G4fAsk",
        "no_args_ai": "‚ùì **Usage:** `.ai <model_alias>`\n**Available:** `{}`\n**Current:** `{}`",
        "invalid_model": "‚ùå Invalid model alias `{}`. Available: {}",
        "model_set": "‚úÖ AI model set to `{}`.",
        "no_args_ask": "‚ùì Please provide a question. Usage: `.ask <query>`",
        "thinking": "üß† Thinking with `{}`...",
        "result": "‚ùì Question:\n{}\n\nüí° Answer ({}):\n{}",
        "missing_dep": "‚ùå **Error:** Missing dependency. Please install required packages (`pip install -U g4f`).",
        "request_error": "‚ùå **Error:** Failed to get response from `{}`. ({})",
        "unexpected_error": "‚ùå **Error:** An unexpected error occurred while contacting `{}`. Error: {}",
    }
    strings_ru = {
        "_cls_doc": "–ú–æ–¥—É–ª—å –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –ò–ò –º–æ–¥–µ–ª—è–º–∏ —á–µ—Ä–µ–∑ g4f",
        "no_args_ai": "‚ùì **–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:** `.ai <–º–æ–¥–µ–ª—å>`\n**–î–æ—Å—Ç—É–ø–Ω—ã–µ:** `{}`\n**–¢–µ–∫—É—â–∞—è:** `{}`",
        "invalid_model": "‚ùå –ù–µ–≤–µ—Ä–Ω–æ–µ –∏–º—è –º–æ–¥–µ–ª–∏ `{}`. –î–æ—Å—Ç—É–ø–Ω—ã–µ: {}",
        "model_set": "‚úÖ –ú–æ–¥–µ–ª—å –ò–ò —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –Ω–∞ `{}`.",
        "no_args_ask": "‚ùì –£–∫–∞–∂–∏—Ç–µ –≤–æ–ø—Ä–æ—Å. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: `.ask <–∑–∞–ø—Ä–æ—Å>`",
        "thinking": "üß† –î—É–º–∞—é —Å –ø–æ–º–æ—â—å—é `{}`...",
        "result": "‚ùì –í–æ–ø—Ä–æ—Å:\n{}\n\nüí° –û—Ç–≤–µ—Ç ({}):\n{}",
        "missing_dep": "‚ùå **–û—à–∏–±–∫–∞:** –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–∞–∫–µ—Ç—ã (`pip install -U g4f`).",
        "request_error": "‚ùå **–û—à–∏–±–∫–∞:** –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç `{}`. ({})",
        "unexpected_error": "‚ùå **–û—à–∏–±–∫–∞:** –ü—Ä–æ–∏–∑–æ—à–ª–∞ –Ω–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ `{}`. –û—à–∏–±–∫–∞: {}",
        "_cmd_doc_ai": "<–º–æ–¥–µ–ª—å> - –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –º–æ–¥–µ–ª—å –ò–ò (gemini, deepseek, claude)",
        "_cmd_doc_ask": "<–∑–∞–ø—Ä–æ—Å> - –ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –ò–ò"
    }

    async def client_ready(self, client, db):
        self.db = db
        self._client = g4f.Client()
        self._default_model = "gemini-2.0-flash"
        self._available_models = {
            "gemini": "gemini-2.0-flash",
            "deepseek": "deepseek-v3",
            "claude": "claude-3.7-sonnet",
        }
        self.db.setdefault("g4f_model", self._default_model)

    @loader.command(
        ru_doc="<–º–æ–¥–µ–ª—å> - –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –º–æ–¥–µ–ª—å –ò–ò (gemini, deepseek, claude)",
        en_doc="<model_alias> - Set the AI model (gemini, deepseek, claude)",
        aliases=["setmodel"],
    )
    async def ai(self, message: Message):
        """<model_alias> - Set the AI model (gemini, deepseek, claude)"""
        args = utils.get_args_raw(message)
        available_keys = ", ".join(self._available_models.keys())

        if not args:
            current_model = self.db.get("G4fAskMod", "g4f_model", self._default_model)
            await utils.answer(
                message, self.strings("no_args_ai").format(available_keys, current_model)
            )
            return

        model_alias = args.lower().strip()
        if model_alias not in self._available_models:
            await utils.answer(
                message, self.strings("invalid_model").format(model_alias, available_keys)
            )
            return

        selected_model = self._available_models[model_alias]
        self.db.set("G4fAskMod", "g4f_model", selected_model)
        await utils.answer(message, self.strings("model_set").format(selected_model))

    @loader.command(
        ru_doc="<–∑–∞–ø—Ä–æ—Å> - –ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –ò–ò",
        en_doc="<query> - Ask a question to the configured AI model",
        aliases=["a"],
    )
    async def ask(self, message: Message):
        """<query> - Ask a question to the configured AI model"""
        prompt = utils.get_args_raw(message)
        if not prompt:
            # Use edit_delete equivalent for temporary messages
            await utils.answer(message, self.strings("no_args_ask"), parse_mode="markdown")
            await asyncio.sleep(5)
            await message.delete()
            # Attempt to delete the bot's response if possible
            # This part is tricky as utils.answer might not return the message object
            # depending on Hikka version/configuration
            return

        model = self.db.get("G4fAskMod", "g4f_model", self._default_model)

        msg = await utils.answer(message, self.strings("thinking").format(model), parse_mode="markdown")
        # Ensure msg is a Message object if utils.answer returns it
        if not isinstance(msg, Message):
            msg = message # Fallback to the original message for context

        try:
            # The chat.completions.create method is not async, so don't use await
            response = self._client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
            )
            answer = response.choices[0].message.content
            await utils.answer(msg, self.strings("result").format(prompt, model, answer), parse_mode="markdown")

        except ImportError:
            logger.exception("Missing dependency for g4f")
            await utils.answer(msg, self.strings("missing_dep"), parse_mode="markdown")
        except Exception as e:
            error_name = type(e).__name__
            if "request" in error_name.lower():
                logger.exception("Request error from g4f")
                await utils.answer(msg, self.strings("request_error").format(model, str(e)), parse_mode="markdown")
            else:
                logger.exception("Failed to get response from AI")
                await utils.answer(msg, self.strings("unexpected_error").format(model, str(e)), parse_mode="markdown") 
